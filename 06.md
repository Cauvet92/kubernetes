# D√©ploiement de microservices Stateless: Introduction

## Bug fix

```bash
rm /var/lib/dpkg/lock-frontend
rm /var/cache/apt/archives/lock
rm /var/cache/debconf/config.dat
rm /var/cache/debconf/passwords.dat 
rm /var/cache/debconf/templates.dat
```


## Pr√©requis

Nous allons utiliser notre serveur de d√©veloppement Ubuntu sur lequel nous avons install√© kubectl et t√©l√©charg√© le fichier kubeconfig.

kubectl est un outil en ligne de commande utilis√© dans la gestion et le d√©ploiement des applications dans les clusters Kubernetes. Il est con√ßu pour faciliter l'interaction entre administrateurs et les clusters Kubernetes et pr√©sente une large gamme de fonctionnalit√©s.

Afin de communiquer avec le serveur API Kubernetes et de g√©rer nos clusters et ressources Kubernetes, nous avons besoin d'installer cette CLI.

```bash
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

```

Activez la fonctionnalit√© d'auto-compl√©tion:

```bash
cat << EOF >> ~/.bashrc
if [ -f /etc/bash_completion ] && ! shopt -oq posix; then
    . /etc/bash_completion
fi
EOF

kubectl completion bash | sudo tee /etc/bash_completion.d/kubectl > /dev/null
sudo chmod a+r /etc/bash_completion.d/kubectl
echo 'source <(kubectl completion bash)' >>~/.bashrc
source ~/.bashrc
```

Copiez le fichier kubeconfig sur votre serveur de d√©veloppement:

```bash
mkdir -p ~/.kube
```

Ajoutez le contenu du fichier yaml dans `~/.kube/config`.

Pour les exemples suivants, nous allons utiliser Python comme langage de programmation principal et Flask comme framework. Python et Flask sont faciles √† comprendre m√™me si vous n'√™tes pas un d√©veloppeur Python. Par cons√©quent, avec ces choix, ce guide sera facile √† lire et √† comprendre pour tous les d√©veloppeurs.

Sur votre serveur de d√©veloppement, commencez par installer:

- pip3: Le gestionnaire de paquets officiel et la commande pip pour Python 3.
- virtualenvwrapper: Un ensemble d'extensions pour cr√©er et supprimer des environnements de d√©veloppement virtuels Python.

```bash
apt update &&  apt install -y python3-pip

pip install virtualenvwrapper
export WORKON_HOME=~/Envs
mkdir -p $WORKON_HOME

cat << EOF >> ~/.bashrc
export VIRTUALENVWRAPPER_PYTHON='/usr/bin/python3'
source /usr/local/bin/virtualenvwrapper.sh
EOF

source ~/.bashrc
```

Cr√©ons un nouvel environnement virtuel:

```bash
mkvirtualenv stateless-flask
```

Cr√©ez ensuite les dossiers pour notre application Flask et installez ses d√©pendances.

```bash
mkdir -p stateless-flask
cd stateless-flask
mkdir -p app
mkdir -p kubernetes
pip install Flask==3.0.0
pip freeze > app/requirements.txt
```

Le code suivant cr√©e une application de todo liste simple:

```python
cat << EOF > app/app.py
from flask import Flask, jsonify, request

app = Flask(__name__)

# Define a list of tasks
tasks = []

# route for getting all tasks
@app.route('/tasks', methods=['GET'])
def get_tasks():
    return jsonify({'tasks': tasks})

# Route for getting a single task
@app.route('/tasks', methods=['POST'])
def add_task():
    task = {
        'id': len(tasks) + 1,
        'title': request.json['title'],
        'description': request.json['description'],
    }
    tasks.append(task)
    return jsonify(task), 201

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)
EOF
```

Ce code cr√©e une application Flask qui d√©finit une todo liste. Il y a deux routes - une pour r√©cup√©rer toutes les t√¢ches, et une autre pour ajouter une t√¢che.

La fonction `get_tasks()` renvoie une r√©ponse JSON contenant toutes les t√¢ches de la liste.

La fonction `add_task()` cr√©e une nouvelle t√¢che avec un ID, un titre et une description, l'ajoute √† la liste des t√¢ches, et renvoie une r√©ponse JSON contenant la nouvelle t√¢che.

Le bloc `if name == 'main':` ex√©cute l'application Flask et la rend disponible sur l'ordinateur local sur le port 5000. Le mode debug est activ√© pour aider au d√©veloppement.

Ensuite, nous allons cr√©er un fichier Dockerfile :

```Dockerfile
cat << EOF > app/Dockerfile
# Use an official Python runtime as a parent image
FROM python:3.9-slim-buster
# Set the working directory to /app
WORKDIR /app
# Copy the current directory contents into the container at /app
COPY . /app
# Install any needed packages specified in requirements.txt
RUN pip install --no-cache-dir -r requirements.txt
# Make port 5000 available to the world outside this container
EXPOSE 5000
# Define environment variable
CMD ["python", "app.py"]
EOF
```

Nous pouvons maintenant construire l'image et l'ex√©cuter pour la tester.

```bash
docker build  -t stateless-flask:v0 -f app/Dockerfile app
docker run -it -p 5000:5000 stateless-flask:v0
```

Puisque nous exposons le conteneur sur le port 5000 de l'h√¥te, nous interrogerons l'API g√©n√©r√©e par Flask en utilisant un outil comme Postman, l'adresse IP publique de notre h√¥te suivi du port 5000.

T√©l√©chargez et installez [Postman](https://www.postman.com/downloads/).

![](resources/images/postmansend.jpg)

Vous pouvez utiliser une requ√™te POST avec les donn√©es suivantes :

```json
{
    "description": "Master the art of using containers, Kubernetes and microservices",    
    "title": "Kubernetes"
}
```

N'oubliez pas d'ajouter l'en-t√™te `Content-Type` en lui attribuant la valeur `application/json`.

Apr√®s avoir enregistr√© le premier √©l√©ment, vous pouvez ex√©cuter une requ√™te GET sur `/tasks`, vous obtiendrez les m√™mes donn√©es JSON que celles que vous avez stock√©es. Cependant, lorsque vous arr√™tez le conteneur, les donn√©es sont effac√©es, ce qui est normal car l'application est stateless et ne stocke rien dans un datastore externe.

Maintenant, cr√©ez un compte sur [Docker Hub](https://hub.docker.com/) et poussez l'image.

```bash
docker login
docker tag stateless-flask:v0 <dockerhub_username>/stateless-flask:v0
docker push <dockerhub_username>/stateless-flask:v0
```

Remplacez `<dockerhub_username>` par votre nom d'utilisateur Docker Hub.

## Cr√©ation d'un namespace

Nous pouvons d√©ployer l'application dans le namespace par d√©faut, mais ce n'est pas recommand√©, surtout lorsque vous avez plusieurs applications en cours d'ex√©cution dans le m√™me cluster. Dans ce cas, l'approche recommand√©e est de cr√©er un namespace distinct pour chaque application.

Nous allons d√©ployer l'API Flask dans un namespace diff√©rent de celui par d√©faut. Cr√©ons le :

```yaml
cat <<EOF > kubernetes/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: stateless-flask
EOF
```

Appliquez maintenant le YAML.

```bash
kubectl apply -f kubernetes/namespace.yaml
```

Affichez la liste de tous les namespaces:

```bash
kubectl get ns
```

Vous devriez √™tre en mesure de voir le nouveau Namespace cr√©√©.

> üí° Un Namespace fournit une s√©paration virtuelle des ressources au sein d'un cluster.
>
> Il permet √† plusieurs √©quipes ou applications d'utiliser le m√™me cluster Kubernetes tout en disposant de leurs propres ensembles isol√©s de ressources telles que les pods, les services et les volumes.
>
> Cette s√©paration virtuelle aide √† la gestion des ressources et √©vite les conflits entre les √©quipes ou les applications.
>
> Par exemple, lorsque vous voulez ex√©cuter plusieurs applications sur un seul cluster, vous pouvez d√©ployer chaque application dans un Namespace diff√©rent et une ressource Ingress qui achemine le trafic vers toutes ces applications dans un autre Namespace.

## Cr√©ation du d√©ploiement

Pour d√©ployer notre conteneur Flask, nous devons cr√©er un d√©ploiement. Le d√©ploiement cr√©era un ReplicaSet et ce dernier sera responsable de maintenir un ensemble stable de Pods.

> üí° Un Pod est l'unit√© la plus petite du mod√®le d'objet Kubernetes, et il repr√©sente une seule instance d'un processus en cours d'ex√©cution dans le cluster.
>
> Un Pod peut contenir un ou plusieurs conteneurs, qui sont garantis de fonctionner ensemble sur le m√™me h√¥te et de partager le m√™me r√©seau et le m√™me espace de stockage.
>
> Tous les conteneurs dans un Pod partagent le m√™me ensemble de namespaces Linux, y compris le r√©seau, ce qui signifie qu'ils peuvent communiquer les uns avec les autres via l'interface r√©seau localhost.
>
> Les conteneurs dans un seul Pod partagent le m√™me cycle de vie, lorsqu'un Pod est cr√©√©, tous les conteneurs sont cr√©√©s et lorsque le Pod est tu√© ou supprim√©, tous les conteneurs √† l'int√©rieur de ce Pod disparaissent.
>
> Par exemple, si "container_a" et "container_b" s'ex√©cutent sur le m√™me Pod, tous deux ont la m√™me adresse IP, qui est l'adresse IP du Pod. "container_a" acc√©dera √† "container_b" comme il acc√©derait √† [localhost](http://localhost/) et vice-versa.  

Dans le manifeste YAML suivant, assurez-vous de remplacer `<votre-registre-docker>` par le nom d'utilisateur de votre Docker Hub.

```yaml
cat << EOF > kubernetes/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stateless-flask
  namespace: stateless-flask
spec:
  replicas: 3
  selector:
    matchLabels:
      app: stateless-flask
  template:
    metadata:
      labels:
        app: stateless-flask
    spec:
      containers:
      - name: stateless-flask
        image: <votre-registre-docker>/stateless-flask:v0
        ports:
        - containerPort: 5000
EOF
```

Voyons ce que le fichier Deployment fait ligne par ligne :

- **`apiVersion`** sp√©cifie la version de l'API Kubernetes √† utiliser pour cet objet, qui est "apps/v1" dans ce cas.

- **`kind`** sp√©cifie le type d'objet √† cr√©er, qui est un d√©ploiement.

- **`metadata`** contient des m√©tadonn√©es sur le d√©ploiement, y compris le nom et le namespace.

- **`spec`** sp√©cifie l'√©tat souhait√© du d√©ploiement. Il contient le nombre de replicas √† cr√©er (dans ce cas, 3), ainsi qu'un s√©lecteur pour correspondre aux labels des pods qui doivent √™tre g√©r√©s par le d√©ploiement.

- Le champ **`selector`** dans un d√©ploiement Kubernetes sp√©cifie les labels utilis√©es pour identifier et s√©lectionner les pods que le d√©ploiement g√®re. Dans notre code, le s√©lecteur sp√©cifie que le d√©ploiement g√©rera des pods avec le label **`app: stateless-flask`**. Cela signifie que tous les pods qui correspondent √† ce label seront inclus dans l'ensemble de replicas maintenu par le d√©ploiement, et tout pod qui ne correspond pas √† ce label ne sera pas inclus. Le champ **`matchLabels`** dans le s√©lecteur sp√©cifie les labels qui doivent correspondre pour qu'un pod soit inclus dans l'ensemble.

- **`template`** contient la sp√©cification des pods qui doivent √™tre cr√©√©s par le d√©ploiement. Il sp√©cifie les labels √† appliquer aux pods, qui doivent correspondre au s√©lecteur dans le **`spec`**.

- **`containers`** contient la liste des conteneurs √† d√©ployer dans les pods. Dans ce cas, il n'a qu'un seul conteneur nomm√© "stateless-flask", qui utilise l'image Docker sp√©cifi√©e dans **`image`** et expose le port 5000.

Nous pouvons appliquer le fichier YAML et obtenir la liste des d√©ploiements dans le namespace "stateless-flask". Assurez-vous de changer `<votre-nom-d-utilisateur-docker-registry>` par votre nom d'utilisateur Docker Hub.

```bash
kubectl apply -f kubernetes/deployment.yaml
kubectl get pods -n stateless-flask
```

Le sch√©ma suivant montre les diff√©rentes ressources Kubernetes que nous avons cr√©√©es.

Notez que notre cluster poss√®de un noeud. Si nous en avions deux, les 3 Pods que nous avons cr√©√©s seraient probablement r√©partis entre les noeuds.

> üí° Un d√©ploiement est responsable de la gestion d'un ensemble de replicas d'un Pod sp√©cifique.
>
> Le d√©ploiement garantit que le nombre d√©sir√© de replicas s'ex√©cute √† tout moment et fournit un moyen de r√©aliser des mises √† jour progressives et des retours en arri√®re.
>
>Il fournit une interface d√©clarative pour d√©ployer, mettre √† jour et supprimer des Pods et des ReplicaSets.

> üí° Un ReplicaSet est un autre objet Kubernetes qui est responsable de maintenir un ensemble stable de r√©plicas de Pods. Il fonctionne en veillant √† ce qu'un nombre sp√©cifique de r√©plicas de Pods bas√© sur un mod√®le d√©fini (g√©n√©ralement d√©fini dans un d√©ploiement) soit en cours d'ex√©cution.
>
> Les ReplicaSets sont souvent utilis√©s en tant que partie d'un d√©ploiement, avec le d√©ploiement g√©rant le ReplicaSet pour assurer le maintien du nombre de replicas souhait√©. Il n'est donc pas recommand√© de cr√©er directement un ReplicaSet. Au lieu de cela, cr√©ez un d√©ploiement.

![](resources/images/deployment.png)

## Examen des Pods et des D√©ploiements

Une commande utile que nous pouvons utiliser pour v√©rifier l'√©tat, les √©v√©nements et la configuration du pod est `kubectl describe`.

Si vous avez utilis√© Docker, vous √™tes certainement familier avec la commande `docker inspect <container>`. La commande `kubectl describe pod <pod>`, est quelque peu similaire.

```bash
kubectl -n stateless-flask describe pod <pod>
```

Remplacez `<pod>` par le nom de n'importe quel Pod. Vous pouvez obtenir la liste des Pods en utilisant la commande `kubectl -n stateless-flask get pods`.

Vous pouvez √©galement d√©crire le d√©ploiement en utilisant :

```bash
kubectl -n stateless-flask describe deployment <nom de d√©ploiement>
```

L'une des informations les plus utiles fournies par la commande `describe` est la liste des √©v√©nements.

Les √©v√©nements fournissent un moyen de surveiller l'√©tat des diff√©rents objets du syst√®me. Ces objets peuvent √™tre des n≈ìuds, des pods, des d√©ploiements, des services, etc. Ils fournissent une trace d'audit des changements qui se produisent sur ces objets au fil du temps.

Les logs des pods sont un autre moyen de comprendre ce qui se passe √† l'int√©rieur d'un pod. √Ä l'aide de la commande suivante, nous pouvons voir les logs d'un pod:

```bash
kubectl -n stateless-flask logs <pod>
```

Vous devez remplacer `<pod>` par le nom d'un Pod.

Si vous voulez obtenir les logs de tous les pods en une seule commande, vous pouvez le faire en filtrant par le label. Voici un exemple :

```bash
kubectl -n stateless-flask logs -l app=stateless-flask
```

Nous utilisons `app=stateless-flask` car c'est ce que nous avons configur√© dans le `template` de nos Pods dans le fichier Deployment (label `app: stateless-flask`)

Pour afficher les logs et suivre les modifications, vous pouvez ajouter le flag `-f`.

```bash
kubectl -n stateless-flask logs <pod> -f
kubectl -n stateless-flask logs -l app=stateless-flask -f
```

## Acc√©der aux Pods

Un Pod n'expose aucune adresse/port externe par d√©faut, par cons√©quent, si nous voulons voir l'application Flask s'ex√©cuter, nous devons transf√©rer le port interne du Pod vers l'un des ports de notre serveur de d√©veloppement. Voici comment proc√©der:

```bash
kubectl port-forward stateless-flask-<pod_id> 5000:5000 -n stateless-flask
```

Assurez-vous de remplacer `stateless-flask-<pod_id>` par le nom de tout Pod. Vous pouvez obtenir la liste des Pods en utilisant `kubectl get pods -n stateless-flask`.

Vous pouvez √©galement utiliser AWK pour obtenir le premier Pod qui appara√Æt sur la liste lorsque vous tapez `kubectl get pods` comme suit:

```bash
export pod=$(kubectl get pods -n stateless-flask | awk 'FNR==2{print $1}' )

kubectl port-forward $pod 5000:5000 -n stateless-flask
```

Dans les deux cas, le Pod est maintenant accessible via notre serveur local, nous pouvons v√©rifier sa r√©ponse en utilisant:

```bash
curl http://0.0.0.0:5000/tasks
```

## Exposition d'un d√©ploiement

Si nous voulons utiliser notre application Flask de mani√®re r√©guli√®re, nous devons la rendre accessible via un navigateur web. Cependant, il n'existe aucun acc√®s externe √† notre Pod, √† moins que nous ne cr√©ions un port de transfert. C'est pourquoi nous avons besoin de cr√©er un Service.

> üí° Un Service est une abstraction puissante qui fournit un moyen transparent d'exposer un d√©ploiement de Pods en tant que service r√©seau.
>
> Un d√©ploiement est l'√©pine dorsale de votre application, g√©rant la cr√©ation et la mise √† l'√©chelle d'un ensemble de Pods identiques avec facilit√©. Pendant ce temps, un Service vous donne une adresse IP fiable et un nom DNS pour acc√©der √† ces Pods.
>
> En d√©finissant un Service pour un d√©ploiement, vous pouvez facilement acc√©der aux pods √† partir d'autres applications au sein du cluster ou de l'ext√©rieur du cluster via l'adresse IP ou le nom DNS du Service.
>
> Ce d√©couplage intelligent des Pods √† partir du Service permet une gestion facile des Pods et offre plus de flexibilit√© dans la fa√ßon dont ils sont accessibles.

Il existe plusieurs fa√ßons d'exposer une application sur le r√©seau, notamment les services ClusterIP, NodePort, LoadBalancer, Ingress et les services Headless.

### Service ClusterIP

ClusterIP est le type de service par d√©faut qui fournit une adresse IP pour atteindre le service au sein du cluster. Il expose les Pods d'un cluster √† d'autres objets dans le m√™me cluster en utilisant une adresse IP interne, permettant une communication transparente entre les composants.

Le service ClusterIP est utilis√© pour permettre la communication entre diff√©rents composants ou microservices au sein d'un cluster Kubernetes. Il agit comme une [adresse IP virtuelle](https://fr.wikipedia.org/wiki/Adresse_IP_viruelle) attribu√©e √† un ensemble de Pods.

Voici √† quoi ressemble un ClusterIP :

![](resources/images/clusterip.png)

Voici comment cr√©er un ClusterIP pour notre service :

```yaml
cat <<EOF > kubernetes/cluserip-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: stateless-flask-clusterip-service
  namespace: stateless-flask
spec:
    type: ClusterIP
    selector:
        app: stateless-flask
    ports:
    - port: 5000
      protocol: TCP
      targetPort: 5000
EOF
```

- **`apiVersion`** : La version de l'API Kubernetes utilis√©e pour ce manifeste.
  
- **`kind`** : Le type d'objet Kubernetes d√©crit par ce manifeste. Dans ce cas, il s'agit d'un Service.
  
- **`metadata`** : Les m√©tadonn√©es sur le Service, y compris son nom et son Namespace.
  
- **`spec`** : La sp√©cification du Service, qui comprend son type, son s√©lecteur et ses ports.
  
- **`type: ClusterIP`** indique que ce Service est un service ClusterIP, ce qui signifie qu'il n'est accessible que dans le cluster.
  
- **`selector:`** d√©finit un s√©lecteur qui identifie les Pods vers lesquels ce Service doit router le trafic. Dans ce cas, il s√©lectionne les Pods avec le label **`app: stateless-flask`**.
  
- **`ports:`** sp√©cifie la configuration de port du Service. Dans ce cas, il expose le port 5000 des Pods en tant que port 5000 du Service.

- **`port`** fait r√©f√©rence au port sur lequel le service √©coutera, tandis que **`targetPort`** fait r√©f√©rence au port sur lequel les Pods backend du Service √©coutent.

Appliquer le fichier YAML et v√©rifier la liste des services:

```bash
kubectl apply -f kubernetes/cluserip-service.yaml
kubectl get svc -n stateless-flask
```

Pour acc√©der √† l'application, nous avons besoin d'utiliser le service ClusterIP. Pour ce faire, nous utiliserons le port-forwarding de Kubernetes:

```bash
kubectl port-forward svc/stateless-flask-clusterip-service 5000:5000 -n stateless-flask
```

Maintenant, nous pouvons tester la connexion au service en utilisant une commande cURL.

```bash
curl http://127.0.0.1:5000/tasks
```

### Service NodePort

NodePort est un service qui expose un d√©ploiement √† l'ext√©rieur sur un port statique sur chaque n≈ìud du cluster. La plage de ports pour les services NodePort est de 30000 √† 32767, ce qui permet au trafic externe d'atteindre le service sur le port sp√©cifi√©, qui est ensuite transf√©r√© au pod.

L'un des avantages de NodePort est sa simplicit√©, car il ne n√©cessite pas de r√©partiteurs de charge externes ou de composants r√©seau. Il est √©galement facile √† utiliser et ne n√©cessite pas beaucoup de configuration.

Cependant, l'utilisation de NodePort pr√©sente quelques inconv√©nients. Il expose le service sur un port statique sur chaque n≈ìud du cluster, ce qui peut causer des probl√®mes de s√©curit√© si vous avez potentiellement des probl√®mes de s√©curit√©. Il n√©cessite √©galement l'ouverture d'une plage de ports sur tous les n≈ìuds du cluster, ce qui peut √™tre un vecteur d'attaque potentiel.

`NodePort` peut √™tre utile √† des fins de test et de d√©bogage. Cependant, si vous avez une charge de travail de production et que vous souhaitez autoriser le trafic externe, ce n'est pas la solution la plus adapt√©e.

![](resources/images/nodeport.png)

Voici comment cr√©er un service `NodePort` :

```yaml
cat <<EOF > kubernetes/nodeport-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: stateless-flask-nodeport-service
  namespace: stateless-flask
spec:
    type: NodePort
    selector:
        app: stateless-flask
    ports:
    - port: 5000
      protocol: TCP
      targetPort: 5000
      nodePort: 30000
EOF
```

Les ports sur lesquels ce service √©coutera sont d√©finis dans **`ports`**, y compris **`port`** (le port sur lequel le service √©coute), **`protocol`** (le protocole utilis√© par le port), **`targetPort`** (le port sur le pod vers lequel le trafic doit √™tre envoy√©) et **`nodePort`** (le port sur lequel le service est expos√© sur chaque n≈ìud du cluster). Dans ce cas, le service √©coute sur le port **`5000`** et est expos√© sur chaque n≈ìud du cluster sur le port **`30000`**.

Maintenant, vous pouvez appliquer ce YAML :

```bash
kubectl apply -f kubernetes/nodeport-service.yaml
```

Listez les services :

```bash
kubectl get svc -n stateless-flask
```

Obtenez l'adresse IP externe de l'un des noeuds de votre cluster :

```bash
kubectl get nodes -o wide
```

√Ä l'aide de votre navigateur Web, d'un outil tel que Postman ou cURL, vous pouvez v√©rifier la r√©ponse de l'API √† l'aide de l'adresse IP externe du noeud et du port 30000.

```bash
curl http://<external_ip>:30000/tasks
```

Si vous voulez tout faire en une seule commande, vous pouvez utiliser :

```bash
curl HTTP://$(kubectl get nodes -o wide | awk '{print $7}' | tail -n +2):30000/tasks
```

`kubectl get nodes -o wide | awk '{print $7}' | tail -n +2` permet d'obtenir l'adresse IP externe sans afficher toute la sortie de `kubectl get nodes -o wide`.

### Service LoadBalancer

LoadBalancer est un autre type de service disponible pour une utilisation dans un cluster Kubernetes.

Il aide √† distribuer le trafic entrant √† plusieurs r√©plica d'un pod.

Il est une impl√©mentation du mod√®le [Load Balancing](https://en.wikipedia.org/wiki/Load_balancing_(computing)), qui est utilis√© pour distribuer le trafic sur plusieurs n≈ìuds, services, ou syst√®mes.

En Kubernetes, les choses sont un peu diff√©rentes : Pour mettre en ≈ìuvre ce mod√®le, vous aurez besoin de deux choses :

- Un service LoadBalancer
- Une machine d'√©quilibrage de charge

Alors que le service est un objet Kubernetes qui fait partie du cluster, la machine est externe et ne fait pas partie du cluster. Si vous g√©rez un cluster Kubernetes sur des serveurs bare-metal ou sur site, la mise en ≈ìuvre peut √™tre difficile. Cependant, si vous utilisez un cluster g√©r√© en utilisant un cloud public tel que AWS, GCP ou DigitalOcean, les choses sont beaucoup plus simples.

Normalement, le fournisseur de cloud provisionne automatiquement la machine d'√©quilibrage de charge pour vous.

Un service d'√©quilibrage de charge peut aider √† am√©liorer la disponibilit√© et la scalabilit√© d'une application en r√©partissant le trafic sur plusieurs replicas, ce qui r√©duit la probabilit√© d'un point de d√©faillance unique.

L'un de ses avantages importants est sa capacit√© √† d√©tecter automatiquement les replicas non fonctionnelles et √† rediriger le trafic vers celles qui sont en bonne sant√©, ce qui ajoute une autre couche de fiabilit√© √† votre charge de travail.

De plus, lorsque vous cr√©ez un √©quilibreur de charge, vous pouvez ajouter des fonctionnalit√©s avanc√©es comme la terminaison SSL et l'affinit√© de session.

Cependant, cr√©er une machine d'√©quilibrage de charge pour chacun de vos services peut √™tre co√ªteux. De plus, ajouter une couche de r√©seau suppl√©mentaire peut introduire une latence, mais dans la plupart des cas, cela n'est pas significatif.

![](resources/images/lb.png)

Pour cr√©er un service LoadBalancer, utilisez le YAML suivant :

```yaml
cat <<EOF > kubernetes/loadbalancer-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: stateless-flask-loadbalancer-service
  namespace: stateless-flask
spec:
    type: LoadBalancer
    selector:
        app: stateless-flask
    ports:
    - port: 5000
      protocol: TCP
      targetPort: 5000
EOF
```

Ici, un seul port est d√©fini avec les propri√©t√©s suivantes :

- **`port`**: Le num√©ro de port sur lequel le service sera disponible.
- **`protocol`**: Le protocole utilis√© par le service, dans ce cas TCP.
- **`targetPort`**: Le num√©ro de port sur lequel le service acheminera le trafic vers les Pods.

Ex√©cutez la commande apply:

```bash
kubectl apply -f kubernetes/loadbalancer-service.yaml
```

Surveillez la liste de vos services en utilisant `kubectl get svc -n stateless-flask -w` et lorsque l'adresse IP externe de votre machine d'√©quilibrage de charge est pr√™te, commencez √† tester l'API en utilisant la m√™me IP et le port 5000 (`port`).

Voyons les tests:

```bash
load_balancer_ip=$(kubectl get svc -n stateless-flask | grep "LoadBalancer" | awk '{print $4}')

curl http://$load_balancer_ip:5000/tasks
```

### Service Headless

Normalement, un service Kubernetes a une adresse IP virtuelle (ClusterIP) et un nom DNS qui r√©sout cette adresse IP. Lorsqu'un client se connecte √† l'adresse IP du service, il est automatiquement redirig√© vers l'un des pods qui soutiennent le service. Que faire si je veux acc√©der √† chaque Pod en utilisant sa propre adresse IP ?

Ceci est possible avec la fonctionnalit√© des services Headless de Kubernetes.

Les services Headless sont cr√©√©s en utilisant la m√™me syntaxe que les services r√©guliers, mais avec l'option **`clusterIP`** d√©finie sur **`"None"`**.

Voici comment cr√©er un service Headless :

```yaml
cat <<EOF > kubernetes/headless-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: stateless-flask-headless-service
  namespace: stateless-flask
spec:
    clusterIP: None
    selector:
        app: stateless-flask
    ports:
    - port: 5000
      protocol: TCP
      targetPort: 5000
EOF
```

Vous pouvez maintenant cr√©er le service :

```bash
kubectl apply -f kubernetes/headless-service.yaml
```

Ensuite, vous pouvez afficher la liste des services :

```bash
kubectl get svc -n stateless-flask
```

Pour acc√©der √† un service Headless depuis l'ext√©rieur du cluster, vous pouvez utiliser l'adresse IP publique d'un des Pods du service suivi du port sp√©cifi√© dans le manifeste Kubernetes que nous avons d√©ploy√© (5000 dans notre cas).

Afin de tester la r√©solution DNS, cr√©ons un Pod temporaire nomm√© `tmp01` qui ex√©cute l'image "[tutum/dnsutils](https://hub.docker.com/r/tutum/dnsutils)". Nous d√©ployons ce Pod car il a dnsutils install√©, qui est un ensemble d'outils que nous pouvons utiliser pour tester la r√©solution DNS.

Nous utiliserons la commande `nslookup` pour tester la r√©solution DNS du service `stateless-flask-headless-service.stateless-flask.svc.cluster.local`.

Notez que le service Headless n'a pas d'adresse IP, mais il est accessible par son nom DNS interne :

```text
stateless-flask-headless-service.stateless-flask.svc.cluster.local
```

D√©ployez le Pod temporaire `tmp01` √† l'aide de kubectl :

```bash
kubectl run tmp01 --image=tutum/dnsutils -- sleep infinity
```

Utilisez `nslookup` pour tester la r√©solution de DNS du service Headless. Nous pouvons ex√©cuter une commande depuis un Pod sans √™tre √† l'int√©rieur de celui-ci √† l'aide de :

```bash
kubectl exec -it tmp01 -- nslookup stateless-flask-headless-service.stateless-flask.svc.cluster.local
```

La sortie devrait √™tre similaire √† :

```bash
Server: <IP of the DNS server>
Address: <IP of the DNS server>#53

Name: stateless-flask-headless-service.stateless-flask.svc.cluster.local
Address: <IP of the Pod 1>
Name: stateless-flask-headless-service.stateless-flask.svc.cluster.local
Address: <IP of the Pod 2>
```

Remarquez comment le serveur DNS renvoie les adresses IP des Pods qui ex√©cutent l'application `stateless-flask`. Un ClusterIP ne retournera jamais les adresses IP des Pods; il ne retournera que son propre adresse IP.

Le clustering de base de donn√©es est un cas d'utilisation courant pour les services Headless. RabbitMQ, MySQL et PostgreSQL sont des exemples d'applications qui peuvent √™tre regroup√©es √† l'aide de services Headless.

**Remarque**: Les noms internes ne sont accessibles que depuis l'int√©rieur du cluster. Ils utilisent le format `<nom-du-service>.<namespace>.svc.cluster.local`. Par exemple, pour acc√©der au service `stateless-flask-headless-service` depuis un autre namespace (diff√©rent de `stateless-flask`), nous utilisons `stateless-flask-headless-service.stateless-flask.svc.cluster.local` o√π `stateless-flask` est le namespace o√π le service s'ex√©cute, `stateless-flask-headless-service` est le nom du service et `svc.cluster.local` est le nom de domaine du cluster.

### Service Ingress

Un Ingress est un composant qui permet aux connexions entrantes d'acc√©der au cluster. Il fournit un moyen de g√©rer l'acc√®s externe aux services du cluster. Au lieu de cr√©er un service LoadBalancer distinct pour chaque service, un Ingress vous permet de d√©finir un ensemble de r√®gles qui acheminent le trafic vers diff√©rents services en fonction de l'h√¥te ou du chemin de la demande.

Les services Ingress simplifient l'acc√®s √† vos services en utilisant une seule adresse IP et des noms de domaine. Il peut √©galement utiliser un certificat SSL / TLS pour crypter le trafic. Il dispose d'une configuration flexible qui vous permet de d√©finir les r√®gles pour acheminer le trafic vers les services.

Par exemple, je peux rediriger le trafic vers le service `my-service` lorsque la demande est faite √† l'h√¥te `myhost.com` et au chemin `/my-service`. Alors que lorsque la demande est faite √† l'h√¥te `myhost.com` et au chemin `/my-other-service`, le trafic sera redirig√© vers le service `my-other-service`.

M√™me si certains fournisseurs de cloud ne prennent pas en charge les services Ingress natifs, vous pouvez toujours utiliser un contr√¥leur Ingress open source ou commercial tel que [Traefik](https://traefik.io/) ou [Nginx](https://www.nginx.com/).

Voici une liste de certains d'entre eux:

- Nginx Ingress : C'est l'un des contr√¥leurs les plus populaires, il utilise le serveur web Nginx pour g√©rer le trafic Ingress.
- Traefik Ingress : Ce contr√¥leur prend en charge plusieurs technologies de proxy et offre une int√©gration facile avec les fournisseurs de services cloud.
- HAProxy Ingress : Ce contr√¥leur utilise le serveur proxy HAProxy pour g√©rer le trafic Ingress.
- Istio Ingress: Istio est une plateforme de service mesh qui offre un contr√¥leur Ingress avec des fonctionnalit√©s de s√©curit√© avanc√©es telles que l'authentification, l'autorisation et le chiffrement.
- Contour Ingress : Ce contr√¥leur utilise le serveur proxy Envoy pour g√©rer le trafic Ingress et offre une int√©gration facile avec les ressources Kubernetes.

Il est √©galement important de noter que les r√®gles de routage sont bas√©es sur la demande HTTP(S), il n'est donc pas possible de router le trafic TCP ou UDP. Si vous souhaitez exposer un port TCP/UDP, vous devrez utiliser un LoadBalancer ou un NodePort.

![](resources/images/ingress.png)

Avant de cr√©er l'Ingress, nettoyons tous les services cr√©√©s pr√©c√©demment:

```bash
kubectl delete -f kubernetes/nodeport-service.yaml
kubectl delete -f kubernetes/loadbalancer-service.yaml
kubectl delete -f kubernetes/cluserip-service.yaml
```

Nous aurons besoin d'un service ClusterIP devant nos Pods. Ce service sera utilis√© par le contr√¥leur Ingress pour router le trafic vers les Pods. Cr√©ons-le.

```yaml
cat <<EOF > kubernetes/stateless-flask-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: stateless-flask
  namespace: stateless-flask
spec:
  selector:
    app: stateless-flask
  ports:
  - name: http
    protocol: TCP
    port: 5000
    targetPort: 5000
EOF

kubectl apply -f kubernetes/stateless-flask-service.yaml
```

Maintenant, cr√©ons la ressource Ingress.

Comme nous l'avons vu, il existe plusieurs contr√¥leurs Ingress disponibles. Nous utiliserons le contr√¥leur Ingress Nginx. Ce contr√¥leur est d√©ploy√© en tant que Pod dans le cluster.

Pour installer le contr√¥leur d'Ingress Nginx, nous utiliserons un chart Helm.

[Helm] (https://helm.sh/) est un gestionnaire de packages pour Kubernetes. Il permet d'installer facilement des applications dans Kubernetes / clusterip-service.yaml.

Commencez par installer Helm sur votre serveur de d√©veloppement:

```bash
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
```

Ensuite, ajoutez le r√©f√©rentiel de contr√¥leur Nginx Ingress :

```bash
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update
helm install nginx-ingress ingress-nginx/ingress-nginx 
```

Il est recommand√© de cr√©er un namespace d√©di√© pour le contr√¥leur Ingress, mais nous allons utiliser le namespace par d√©faut pour simplifier les choses.

Si vous voulez utiliser Helm pour installer le contr√¥leur Ingress dans un namespace d√©di√©, vous pouvez utiliser la commande suivante :

```bash
helm install nginx-ingress ingress-nginx/ingress-nginx --namespace ingress-nginx
```

La commande `helm install` installe le contr√¥leur Nginx Ingress √† partir du r√©f√©rentiel des charts stables.

Il d√©finit le param√®tre `publishService` sur `true`, ce qui signifie que le contr√¥leur Ingress sera publi√© en tant que service.

Comme nous n'avons pas sp√©cifi√© de namespace, Helm installera le contr√¥leur dans le namespace default.

Maintenant, nous pouvons lister les Pods et les Services dans le namespace par default:

```bash
kubectl get pods -n default
# or kubectl get pods
kubectl get services -n default
# or kubectl get services
```

Le contr√¥leur Nginx Ingress est d√©ploy√© en tant que Pod dans le namespace default. Il est √©galement publi√© en tant que service dans le namespace default.

Nous devons configurer le contr√¥leur Ingress pour utiliser le service ClusterIP que nous avons cr√©√© plus t√¥t. Pour ce faire, nous devons cr√©er la ressource Ingress.

Notez que le contr√¥leur Ingress n'est pas la ressource Ingress. La ressource Ingress est une ressource Kubernetes qui d√©finit les r√®gles de routage et est utilis√©e par le contr√¥leur Ingress pour router le trafic vers les pods via le service ClusterIP.

Voici un exemple de ressource Ingress :

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
  - host: <host>
    http:
      paths:
      - path: <path>
        pathType: Prefix
        backend:
          service:
            name: <service-name>
            port:
              number: 5000
  ingressClassName: nginx
```

- La section `spec` d√©finit les r√®gles d'entr√©e. Dans ce cas, il y a une seule r√®gle qui correspond √† un nom d'h√¥te `<host>` sp√©cifique et un pr√©fixe de chemin `<path>`.
  
- La section `backend` sp√©cifie vers quel service Kubernetes router le trafic pour les demandes correspondant √† l'h√¥te et au chemin sp√©cifi√©s. Dans ce cas, il sp√©cifie un service nomm√© `<service-name>` sur le port 5000.

- Le champ `pathType` est d√©fini sur `Prefix`, ce qui signifie que le chemin `/tasks` et tout chemin commen√ßant par `/tasks/` correspondra √† cette r√®gle.

- Le champ `ingressClassName` sp√©cifie le nom de la classe Ingress √† utiliser pour cette entr√©e. Dans ce cas, il sp√©cifie le contr√¥leur Ingress Nginx.

Nous pouvons √©galement ajouter un champ ¬´¬†namespace¬†¬ª √† la section ¬´¬†metadata¬†¬ª pour sp√©cifier le namespace dans lequel le service est d√©ploy√©.

Si nous utilisons la ressource ¬´¬†Ingress¬†¬ª ci-dessus, nous aurons un Ingress dans le namespace par d√©faut, alors que notre application est d√©ploy√©e dans le namespace ¬´¬†stateless-flask¬†¬ª. Cela pose un probl√®me car le contr√¥leur d'Ingress ne pourra pas acheminer le trafic vers les Pods.

Pour r√©soudre ce probl√®me, nous devons cr√©er un service ¬´¬†ExternalName¬†¬ª dans le namespace ¬´¬†stateless-flask¬†¬ª qui pointe vers le service ¬´¬†ClusterIP¬†¬ª dans le namespace par d√©faut. Le service ¬´¬†ExternalName¬†¬ª est un type sp√©cial de service qui permet de router le trafic vers un service dans un autre namespace.

Cr√©ez ce fichier:

```yaml
cat <<EOF > kubernetes/stateless-flask-service-externalname.yaml
apiVersion: v1
kind: Service
metadata:
  name: stateless-flask-service-externalname
  namespace: default
spec:
  type: ExternalName
  externalName: stateless-flask.stateless-flask.svc.cluster.local
EOF

```

Et appliquez-le:

```bash
kubectl apply -f kubernetes/stateless-flask-service-externalname.yaml
```

V√©rifiez que le service ExternalName est cr√©√©:

```bash
kubectl get services -n default
```

Maintenant, nous pouvons cr√©er la ressource Ingress:

```bash
export ingress_ip=$(kubectl get services nginx-ingress-ingress-nginx-controller | awk '{print $4}' | tail -n 1)
```

```yaml
cat <<EOF > kubernetes/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
  - host: stateless-flask.${ingress_ip}.nip.io
    http:
      paths:
      - path: /tasks
        pathType: Prefix
        backend:
          service:
            name: stateless-flask-service-externalname
            port:
              number: 5000
  ingressClassName: nginx
EOF
```

Dans la ressource Ingress ci-dessus, nous avons sp√©cifi√© le nom d'h√¥te `stateless-flask.<ingress-ip>.nip.io`. Assurez-vous de changer `<ingress-ip>` avec l'adresse IP du contr√¥leur Ingress. Vous pouvez obtenir l'adresse IP du contr√¥leur Ingress avec la commande suivante:

```bash
kubectl get services nginx-ingress-ingress-nginx-controller | awk '{print $4}' | tail -n 1
```

L'ingress fonctionne avec un DNS public, cependant, nous n'en avons pas. Nous utiliserons [nip.io](https://nip.io/) pour cr√©er un nom de sous-domaine √† la vol√©e √† partir d'une adresse IP. Vous √™tes libre d'acheter et d'utiliser tout autre nom de domaine aupr√®s d'un fournisseur ou d'utiliser un autre service gratuit comme [sslip.io](https://sslip.io/).

Appliquez la ressource Ingress :

```bash
kubectl apply -f kubernetes/ingress.yaml
```

Testez la ressource Ingress :

```bash
kubectl get ingress
```

Vous devriez voir la ressource Ingress dans la sortie. Le champ `ADDRESS` doit contenir l'adresse IP du contr√¥leur Ingress.

Maintenant, vous pouvez ouvrir votre navigateur et aller √†

```text
http://stateless-flask.<ingress-ip>.nip.io/tasks
```

pour voir la liste des t√¢ches.

Si vous avez d'autres cas d'utilisation, vous pouvez consulter la [documentation officielle](https://kubernetes.github.io/ingress-nginx/user-guide/basic-usage/) pour plus de d√©tails.
